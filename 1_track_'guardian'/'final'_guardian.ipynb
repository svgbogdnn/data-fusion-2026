{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544516c5",
   "metadata": {},
   "source": [
    "# Data Fusion Contest 2026 - Ð—Ð°Ð´Ð°Ñ‡Ð° 1 \"Ð¡Ñ‚Ñ€Ð°Ð¶\"\n",
    "\n",
    "Ð£Ñ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ°Ð¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ñƒ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð±Ð°Ð½ÐºÐ° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð² Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÐºÐ°Ð½Ð°Ð»Ð°Ñ….\n",
    "\n",
    "## ÐŸÐ¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð·Ð°Ð´Ð°Ñ‡Ð¸\n",
    "\n",
    "ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ñƒ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ð¾ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°Ð¼, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ð¼ Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒÑŽ, Ð±Ñ‹Ð»Ð¸ Ð½Ðµ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ñ‹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð±Ð°Ð½ÐºÐ°. Ð¢Ð¾ ÐµÑÑ‚ÑŒ, ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ°Ð¼ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð¾Ð¸Ñ‚ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼, Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÑŽÑ‰Ð¸Ð¹ ÑÐ´Ñ€Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð°Ð½Ñ‚Ð¸Ñ„Ñ€Ð¾Ð´Ð° Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹.\n",
    "\n",
    "Ð”Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ 100,000 ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð·Ð° 1.5 Ð³Ð¾Ð´Ð°, Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð½Ð° 4 Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°:\n",
    "\n",
    "1. **Pre-train**, Ñ `2023-10-01` Ð¿Ð¾ `2024-09-30`. Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð². ÐÐµ Ð¸Ð¼ÐµÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¸ Ð¿Ð¾ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸ÑŽ.\n",
    "2. **Train**, Ñ `2024-10-01` Ð¿Ð¾ `2025-05-31`. Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², Ð² ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñƒ Ð½Ð¸Ñ… Ð¿Ð¾ÑÐ²Ð»ÑÑŽÑ‚ÑÑ Ð½ÐµÐ¶ÐµÐ»Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸. Ð˜Ð¼ÐµÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÑƒ ÐºÐ°Ðº Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ (ðŸ”´ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ ÐºÐ»Ð°ÑÑ, Â«ÐºÑ€Ð°ÑÐ½Ñ‹Ð¹ ÑÐ²ÐµÑ‚Â»), Ñ‚Ð°Ðº Ð¸ Ð¿Ð¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹, Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°Ð¼Ð¸ (ðŸŸ¡ Â«Ð¶ÐµÐ»Ñ‚Ñ‹Ð¹ ÑÐ²ÐµÑ‚Â», Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ð¼ ÐºÐ»Ð°ÑÑÐ¾Ð¼). Ð’ÑÐµ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð±ÐµÐ· Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° ÑÑ‚Ð¾Ð¸Ñ‚ ÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ð¼Ð¸ (ðŸŸ¢ Â«Ð·ÐµÐ»ÐµÐ½Ñ‹Ð¹ ÑÐ²ÐµÑ‚Â»).\n",
    "3. **Pre-test**, Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`. Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ ÑÐ°Ð¼Ð¸Ñ… Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¼ÐµÑ‚Ð¾Ðº. ÐÐµ Ð¸Ð¼ÐµÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¸, Ñ‚Ð°Ðº ÐºÐ°Ðº ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ñ‡Ð°ÑÑ‚ÑŒÑŽ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "4. **Test**, Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`. Ð—Ð°ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ°Ð¶Ð´ÑƒÑŽ Ð¸Ð· ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð² Ñ€Ð°Ð¼ÐºÐ°Ñ… ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ. Ð£ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° ÑÑ‚Ð¾Ñ‚ Ð·Ð°ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ð²Ñ‹Ð±Ñ€Ð°Ð½ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾.\n",
    "\n",
    "Ð£ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸Ð¼ÐµÐµÑ‚ÑÑ Ñ€ÑÐ´ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð»Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¾Ð² Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…:\n",
    "\n",
    "- Ð¦ÐµÐ»ÐµÐ²Ð¾Ð¹ ÐºÐ»Ð°ÑÑ Ð¾Ñ‡ÐµÐ½ÑŒ Ñ€ÐµÐ´ÐºÐ¸Ð¹: Ð² Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²ÑÐµÐ³Ð¾ 51 Ñ‚Ñ‹ÑÑÑ‡Ð° Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ (ðŸ”´) Ð¸ 36 Ñ‚Ñ‹ÑÑÑ‡ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… (ðŸŸ¡).\n",
    "- ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð² Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð¶ÐµÐ½Ð½Ð¾Ð¹ Ðº Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ, Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. ÐšÐ°Ðº ÐµÑÐ»Ð¸ Ð±Ñ‹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð·Ð´ÐµÑÑŒ Ð¸ ÑÐµÐ¹Ñ‡Ð°Ñ, Ð° Ð½Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ‡Ð°Ð» ÑƒÐ¶Ðµ Ñ€Ð°Ð½ÐµÐµ ÑÐ»ÑƒÑ‡Ð¸Ð²ÑˆÐ¸ÐµÑÑ Ð¸Ð½Ñ†Ð¸Ð´ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÑÑ‚-Ñ„Ð°ÐºÑ‚ÑƒÐ¼.\n",
    "- Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð½Ñ‹Ð¹ Ð¾Ð±ÑŠÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð·Ð°Ð´Ð°Ñ‡Ðµ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐ°ÐµÑ‚ 200 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹. ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½ÐµÐµ Ð¿Ñ€Ð¾ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ Â«Ð”Ð°Ð½Ð½Ñ‹ÐµÂ»."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76122f85",
   "metadata": {},
   "source": [
    "## Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹\n",
    "\n",
    "Ð­Ñ‚Ð¾ Ñ‚Ð°Ð±Ð»Ð¸Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¾Ð¹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²Ð°Ð¼ `.csv` Ñ„Ð°Ð¹Ð»Ð°. Ð’Ð°Ð¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ð¹ Ð¿Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¼ Ð² Ñ€Ð°Ð¼ÐºÐ°Ñ… ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼, ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ `.csv` Ñ„Ð°Ð¹Ð» Ñ Ð´Ð²ÑƒÐ¼Ñ ÑÑ‚Ð¾Ð»Ð±Ñ†Ð°Ð¼Ð¸:\n",
    "\n",
    "```text\n",
    "event_id,predict\n",
    "125854726334416,-0.338988\n",
    "125949211749418,-4.100378\n",
    "...\n",
    "124738035029214,0.004335\n",
    "```\n",
    "\n",
    "- `event_id` â€” Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸;\n",
    "- `predict` â€” Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°.\n",
    "\n",
    "ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð´Ð»Ñ Ð²ÑÐµÑ… 633,683 Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
    "ÐŸÑ€Ð¸Ð¼ÐµÑ€ `sample_submit.csv` Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ Â«Ð”Ð°Ð½Ð½Ñ‹ÐµÂ».\n",
    "\n",
    "## ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹\n",
    "\n",
    "Ð ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑŽÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿ÑƒÑ‚ÐµÐ¼ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¸Ð½Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¾Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑÐ¾Ð² Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð¿Ð¾ ÐºÐ»Ð°ÑÑÑƒ Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ (ðŸ”´) Ð² Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿ÐµÑ€Ð¸Ð¾Ð´ Ð´Ð°Ð½Ð½Ñ‹Ñ…. Ð˜ÑÑ‚Ð¸Ð½Ð½Ð°Ñ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°Ð¼.\n",
    "\n",
    "ÐœÐµÑ‚Ñ€Ð¸ÐºÐ° ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ â€” **PR-AUC** (Area Under Precision-Recall Curve; Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ Ð¿Ð¾Ð´ ÐºÑ€Ð¸Ð²Ð¾Ð¹ Â«Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ð°-Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒÂ») Ð¿Ð¾ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼.\n",
    "\n",
    "Ð”Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐµÑ‘ `sklearn` Ð¸Ð¼Ð¿Ð»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import average_precision_score\n",
    "```\n",
    "\n",
    "ÐžÐ±Ñ€Ð°Ñ‚Ð¸Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ, Ñ‡Ñ‚Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ `auc` Ð´Ð»Ñ ÑÐ°Ð¼Ð¾Ð¹ Precision-Recall ÐºÑ€Ð¸Ð²Ð¾Ð¹ ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°Ð¼Ð¸ sklearn Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÑ‚ Ðº Ð·Ð°Ð²Ñ‹ÑˆÐµÐ½Ð¸ÑŽ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð²Ð²Ð¸Ð´Ñƒ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ñ‹ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° (Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ð¾Ð»ÑÑ†Ð¸Ð¸ Ñ‚Ð¾Ñ‡ÐµÐº ÐºÑ€Ð¸Ð²Ð¾Ð¹).\n",
    "\n",
    "Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‚ ÑÐ¾Ð±Ð¾Ð¹ 70 Ð´Ð½ÐµÐ¹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹, ÑÐ³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð² 10 Ð½ÐµÐ´ÐµÐ»ÑŒ.\n",
    "ÐžÑ‚ÑÑ‡ÐµÑ‚ Ð¸Ð´ÐµÑ‚ Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`, Ð¿ÐµÑ€Ð²Ð°Ñ Ð½ÐµÐ´ÐµÐ»Ñ Ð¸Ð´ÐµÑ‚ Ñ `2025-06-01` Ð¿Ð¾ `2025-06-07`, Ð¸ Ñ‚.Ð´.\n",
    "\n",
    "Ð¡Ð¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ public/private Ð² ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ 30/70 Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¾ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ (Ð¿Ð¾ Ð½ÐµÐ´ÐµÐ»ÑÐ¼):\n",
    "\n",
    "- ÐÐµÐ´ÐµÐ»Ð¸ 1, 3 Ð¸ 5 Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ð»Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° public Ð»Ð¸Ð´ÐµÑ€Ð±Ð¾Ñ€Ð´Ðµ.\n",
    "- ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ 7 Ð½ÐµÐ´ÐµÐ»ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ð»Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° private Ð»Ð¸Ð´ÐµÑ€Ð±Ð¾Ñ€Ð´Ðµ.\n",
    "- ÐŸÐ¾Ð±ÐµÐ´Ð¸Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÑŽÑ‚ÑÑ Ð¿Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼ Ð½Ð° private Ð»Ð¸Ð´ÐµÑ€Ð±Ð¾Ñ€Ð´Ðµ.\n",
    "- Ð”Ð»Ñ private Ð»Ð¸Ð´ÐµÑ€Ð±Ð¾Ñ€Ð´Ð° Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð´Ð¾ 2-ÑƒÑ… Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4e349",
   "metadata": {},
   "source": [
    "## Ð”Ð°Ð½Ð½Ñ‹Ðµ\n",
    "\n",
    "Ð”Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ 1 Â«Ð¡Ñ‚Ñ€Ð°Ð¶Â» Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ð±Ð¾Ñ€Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¾Ð². Ð”Ð°Ð½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ñ‹ Ð½Ð° 4 Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°:\n",
    "\n",
    "1. **Pre-train**, Ñ `2023-10-01` Ð¿Ð¾ `2024-09-30`. Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ°Ð¼Ð¸Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹.\n",
    "2. **Train**, Ñ `2024-10-01` Ð¿Ð¾ `2025-05-31`. Ð˜ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹, Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ° Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ðº Ð½Ð¸Ð¼.\n",
    "3. **Pre-test**, Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`. ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÐµÐ¹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹, Ð½Ð¾ Ð±ÐµÐ· Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¸ Ðº Ð½Ð¸Ð¼.\n",
    "4. **Test**, Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`. Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², Ð¿Ð¾ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÑƒ Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ ÐµÐµ Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ.\n",
    "\n",
    "Ð”Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð², **pre-train** Ð¸ **train** Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ñ‹ Ð½Ð° 3 Ñ‡Ð°ÑÑ‚Ð¸. ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ ÑÐ²Ð¾ÑŽ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¸Ñ… `customer_id`. Ð¡Ð°Ð¼Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð¿Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°Ð¼ `customer_id` Ð¸ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ `event_dttm`.\n",
    "\n",
    "## Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð·Ð° Pre-train Ð¸ Train\n",
    "\n",
    "- `pretrain_part_1.parquet` â€” 625.2MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2023-10-01` Ð¿Ð¾ `2024-09-30`\n",
    "- `pretrain_part_2.parquet` â€” 622.8MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2023-10-01` Ð¿Ð¾ `2024-09-30`\n",
    "- `pretrain_part_3.parquet` â€” 622.8MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2023-10-01` Ð¿Ð¾ `2024-09-30`\n",
    "- `train_part_1.parquet` â€” 688.1MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2024-10-01` Ð¿Ð¾ `2025-05-31`\n",
    "- `train_part_2.parquet` â€” 686.5MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2024-10-01` Ð¿Ð¾ `2025-05-31`\n",
    "- `train_part_3.parquet` â€” 688.1MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ‚Ñ€ÐµÑ‚Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2024-10-01` Ð¿Ð¾ `2025-05-31`\n",
    "\n",
    "## Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð·Ð° Pre-test Ð¸ Test\n",
    "\n",
    "- `pretest.parquet` â€” 338.9MB, Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`, Ð¿Ñ€ÐµÐ´ÑˆÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¸Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¼Ñƒ Ð´Ð½ÑŽ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "- `test.parquet` â€” 17.2MB, Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ñ `2025-06-01` Ð¿Ð¾ `2025-08-09`, Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð· Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²\n",
    "\n",
    "## ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ\n",
    "\n",
    "- `customer_id` â€” id ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð±Ð°Ð½ÐºÐ°\n",
    "- `event_id` â€” id Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `target` â€” Ñ†ÐµÐ»ÐµÐ²Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ, ÐºÐ»Ð°ÑÑ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸:\n",
    "  - `1` ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ð¾Ð¹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ (ðŸ”´)\n",
    "  - `0` ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð¼ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ (ðŸŸ¡)\n",
    "  - Ð’ÑÐµ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÑ‡Ð¸Ñ‚Ð°ÑŽÑ‚ÑÑ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð±ÐµÐ· Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ (ðŸŸ¢)\n",
    "- `train_labels.parquet` â€” Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ° Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Train)\n",
    "- `sample_submit.csv` â€” Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ\n",
    "\n",
    "## Ð“Ð»Ð¾ÑÑÐ°Ñ€Ð¸Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ð± Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÑ… ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²\n",
    "\n",
    "- `customer_id` â€” id ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð±Ð°Ð½ÐºÐ°\n",
    "- `event_id` â€” id Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `event_dttm` â€” Ð´Ð°Ñ‚Ð°/Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `event_type_nm` â€” Ñ‚Ð¸Ð¿ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `event_desc` â€” Ð·Ð°ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `channel_indicator_type` â€” ÐºÐ°Ð½Ð°Ð» ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `channel_indicator_subtype` â€” Ð¿Ð¾Ð´Ñ‚Ð¸Ð¿ ÐºÐ°Ð½Ð°Ð»Ð° ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `operaton_amt` â€” ÑÑƒÐ¼Ð¼Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð² Ñ€ÑƒÐ±Ð»ÑÑ…\n",
    "- `currency_iso_cd` â€” Ð²Ð°Ð»ÑŽÑ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `mcc_code` â€” Ð³Ñ€ÑƒÐ¿Ð¿Ð° MCC merchant_category_code\n",
    "- `pos_cd` â€” Ð·Ð°ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ point of sale condition code\n",
    "- `accept_language` â€” ÑÐ·Ñ‹Ðº Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° http Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°\n",
    "- `browser_language` â€” ÑÐ·Ñ‹Ðº Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°\n",
    "- `timezone` â€” Ñ‡Ð°ÑÐ¾Ð²Ð¾Ð¹ Ð¿Ð¾ÑÑ\n",
    "- `session_id` â€” Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ ÑÐµÑÑÐ¸Ð¸\n",
    "- `operating_system_type` â€” Ð·Ð°ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹\n",
    "- `battery` â€” Ð·Ð°Ñ€ÑÐ´ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°\n",
    "- `device_system_version` â€” Ð²ÐµÑ€ÑÐ¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹\n",
    "- `screen_size` â€” Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ ÑÐºÑ€Ð°Ð½Ð°\n",
    "- `developer_tools` â€” Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ° (Ñ„Ð»Ð°Ð³ Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ðµ)\n",
    "- `phone_voip_call_state` â€” Ñ„Ð»Ð°Ð³ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ VoIP-Ð·Ð²Ð¾Ð½ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸\n",
    "- `web_rdp_connection` â€” Ñ„Ð»Ð°Ð³ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð½Ð°Ð´ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾Ð¼\n",
    "- `compromised` â€” Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Root-Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ðµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbeddc4",
   "metadata": {},
   "source": [
    "## 0) Environment setup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6b2147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: polars in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (1.38.1)\n",
      "Requirement already satisfied: pandas in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: pyarrow in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (23.0.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (4.67.3)\n",
      "Requirement already satisfied: scipy in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from lightgbm) (1.17.0)\n",
      "Requirement already satisfied: polars-runtime-32==1.38.1 in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from polars) (1.38.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.3.0 in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U lightgbm polars pandas numpy pyarrow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7609d",
   "metadata": {},
   "source": [
    "## 1) Imports and run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR = D:\\Train Data\\data fusion 2026\n",
      "OUT_DIR  = D:\\Apps\\PyCharm\\pythonProject\\fusion_ipynb\\outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\PyCharm\\pythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    DATA_DIR_LOCAL: Path = Path(r\"D:\\Train Data\\data fusion 2026\")\n",
    "    DATA_DIR_KAGGLE: Path = Path(\"/kaggle/input/fusion\")  # Kaggle path: /kaggle/input/<dataset_name>\n",
    "    DATA_DIR: Path = DATA_DIR_KAGGLE if DATA_DIR_KAGGLE.exists() else DATA_DIR_LOCAL\n",
    "\n",
    "    OUT_DIR: Path = Path(os.environ.get(\"OUT_DIR\", \"./outputs\"))\n",
    "\n",
    "    PRETRAIN_FILES: List[str] = field(default_factory=lambda: [\n",
    "        \"pretrain_part_1.parquet\",\n",
    "        \"pretrain_part_2.parquet\",\n",
    "        \"pretrain_part_3.parquet\",\n",
    "    ])\n",
    "    TRAIN_FILES: List[str] = field(default_factory=lambda: [\n",
    "        \"train_part_1.parquet\",\n",
    "        \"train_part_2.parquet\",\n",
    "        \"train_part_3.parquet\",\n",
    "    ])\n",
    "    TRAIN_LABELS_FILE: str = \"train_labels.parquet\"\n",
    "    PRETEST_FILE: str = \"pretest.parquet\"\n",
    "    TEST_FILE: str = \"test.parquet\"\n",
    "\n",
    "    CUSTOMER_COL: str = \"customer_id\"\n",
    "    EVENT_ID_COL: str = \"event_id\"\n",
    "    DT_COL: str = \"event_dttm\"\n",
    "    TARGET_COL: str = \"target\"\n",
    "    Y_COL: str = \"y\"\n",
    "\n",
    "    GREEN_SAMPLE_FRAC: float = 0.02\n",
    "    GREEN_MOD_BASE: int = 1000\n",
    "\n",
    "    TARGET_ENCODER_COLS: Tuple[str, ...] = (\n",
    "        \"event_type_nm\",\n",
    "        \"channel_indicator_type\",\n",
    "        \"currency_iso_cd\",\n",
    "        \"operating_system_type\",\n",
    "    )\n",
    "    TARGET_ENCODER_ALPHA: float = 25.0\n",
    "\n",
    "    COMBO_FREQ_COL_PAIRS: Tuple[Tuple[str, str], ...] = (\n",
    "        (\"event_type_nm\", \"channel_indicator_type\"),\n",
    "        (\"event_type_nm\", \"currency_iso_cd\"),\n",
    "        (\"channel_indicator_type\", \"operating_system_type\"),\n",
    "        (\"event_type_nm\", \"pos_cd\"),\n",
    "    )\n",
    "    COMBO_FREQ_MAX_UNIQUE: int = 120_000\n",
    "\n",
    "    MAX_ROWS_PER_FILE: Optional[int] = None\n",
    "    # --- split ---\n",
    "    VALID_FROM: str = \"2025-05-01\"\n",
    "    SEED: int = 42\n",
    "    NUM_BOOST_ROUND: int = 2200\n",
    "    EARLY_STOPPING: int = 120\n",
    "\n",
    "    # --- optional: customer aggregates from pretrain ---\n",
    "    BUILD_PRETRAIN_CUSTOMER_FEATS: bool = True\n",
    "    PRETRAIN_CUST_FEATS_CACHE: str = \"cust_feats_pretrain.parquet\"\n",
    "\n",
    "\n",
    "CFG = CFG()\n",
    "max_rows_override = os.environ.get(\"MAX_ROWS_PER_FILE\")\n",
    "if max_rows_override:\n",
    "    CFG.MAX_ROWS_PER_FILE = int(max_rows_override)\n",
    "    print(\"MAX_ROWS_PER_FILE override:\", CFG.MAX_ROWS_PER_FILE)\n",
    "\n",
    "pretrain_override = os.environ.get(\"BUILD_PRETRAIN_CUSTOMER_FEATS\")\n",
    "if pretrain_override:\n",
    "    CFG.BUILD_PRETRAIN_CUSTOMER_FEATS = pretrain_override.lower() not in {\"0\", \"false\", \"no\"}\n",
    "    print(\"BUILD_PRETRAIN_CUSTOMER_FEATS override:\", CFG.BUILD_PRETRAIN_CUSTOMER_FEATS)\n",
    "\n",
    "CFG.OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"DATA_DIR =\", CFG.DATA_DIR.resolve())\n",
    "print(\"OUT_DIR  =\", CFG.OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f283c4b",
   "metadata": {},
   "source": [
    "## 2) Utility functions for loading, feature prep, and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f8d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_exist(fp: Path) -> None:\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {fp}\")\n",
    "\n",
    "\n",
    "def pick_amount_col(cols: List[str]) -> Optional[str]:\n",
    "    candidates = [\"operaton_amt\", \"operation_amt\", \"operaton_amt_rub\", \"amount\", \"amt\"]\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def select_existing(lf: pl.LazyFrame, cols: List[str]) -> pl.LazyFrame:\n",
    "    schema = lf.collect_schema()\n",
    "    existing = [c for c in cols if c in schema]\n",
    "    return lf.select(existing)\n",
    "\n",
    "\n",
    "def ensure_datetime(lf: pl.LazyFrame, col: str) -> pl.LazyFrame:\n",
    "    schema = lf.collect_schema()\n",
    "    if col not in schema:\n",
    "        return lf\n",
    "    if schema[col] == pl.Utf8:\n",
    "        return lf.with_columns(pl.col(col).str.strptime(pl.Datetime, strict=False).alias(col))\n",
    "    return lf\n",
    "\n",
    "\n",
    "def add_time_features_pd(df: pd.DataFrame, dt_col: str) -> pd.DataFrame:\n",
    "    dt = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    df[\"hour\"] = dt.dt.hour.astype(\"Int16\")\n",
    "    df[\"dow\"] = dt.dt.dayofweek.astype(\"Int16\")\n",
    "    df[\"dom\"] = dt.dt.day.astype(\"Int16\")\n",
    "    df[\"month\"] = dt.dt.month.astype(\"Int16\")\n",
    "    df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(\"Int8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_amount_features_pd(df: pd.DataFrame, amt_col: str) -> pd.DataFrame:\n",
    "    amt = pd.to_numeric(df[amt_col], errors=\"coerce\")\n",
    "    df[\"amt\"] = amt.astype(\"float32\")\n",
    "    df[\"amt_abs_log1p\"] = np.log1p(np.abs(amt)).astype(\"float32\")\n",
    "    df[\"amt_sign\"] = np.sign(amt.fillna(0)).astype(\"int8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def infer_cat_cols(df: pd.DataFrame, exclude: set) -> List[str]:\n",
    "    cat_cols: List[str] = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        dtype = df[c].dtype\n",
    "        if (\n",
    "            pd.api.types.is_object_dtype(dtype)\n",
    "            or pd.api.types.is_string_dtype(dtype)\n",
    "            or isinstance(dtype, pd.CategoricalDtype)\n",
    "            or pd.api.types.is_bool_dtype(dtype)\n",
    "        ):\n",
    "            cat_cols.append(c)\n",
    "    return cat_cols\n",
    "\n",
    "\n",
    "def _as_string_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(\"string\").fillna(\"__NA__\")\n",
    "\n",
    "\n",
    "def fit_freq_encoders(\n",
    "    train_df: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    *,\n",
    "    max_unique: Optional[int] = 150_000,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    encoders: Dict[str, Dict[str, float]] = {}\n",
    "    for c in tqdm(cat_cols, desc=\"Fitting freq encoders\", leave=False):\n",
    "        if c not in train_df.columns:\n",
    "            continue\n",
    "        s = _as_string_series(train_df[c])\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        if max_unique is not None and len(vc) > max_unique:\n",
    "            vc = vc.iloc[:max_unique]\n",
    "        encoders[c] = {k: float(math.log1p(v)) for k, v in vc.items()}\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def apply_freq_encoders(df: pd.DataFrame, encoders: Dict[str, Dict[str, float]]) -> pd.DataFrame:\n",
    "    for c, mapping in tqdm(encoders.items(), desc=\"Applying freq encoders\", leave=False):\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = _as_string_series(df[c])\n",
    "        df[f\"{c}__freq\"] = s.map(mapping).fillna(0.0).astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_combo_freq_encoders(\n",
    "    df: pd.DataFrame,\n",
    "    col_pairs: List[Tuple[str, str]],\n",
    "    *,\n",
    "    max_unique: Optional[int] = 150_000,\n",
    ") -> Dict[Tuple[str, str], Dict[Tuple[str, str], float]]:\n",
    "    encoders: Dict[Tuple[str, str], Dict[Tuple[str, str], float]] = {}\n",
    "    for c1, c2 in col_pairs:\n",
    "        if c1 not in df.columns or c2 not in df.columns:\n",
    "            continue\n",
    "        s1 = _as_string_series(df[c1]).tolist()\n",
    "        s2 = _as_string_series(df[c2]).tolist()\n",
    "        combos = list(zip(s1, s2))\n",
    "        if not combos:\n",
    "            continue\n",
    "        counts = pd.Series(combos).value_counts()\n",
    "        if max_unique is not None and len(counts) > max_unique:\n",
    "            counts = counts.iloc[:max_unique]\n",
    "        encoders[(c1, c2)] = {\n",
    "            pair: float(math.log1p(cnt)) for pair, cnt in counts.items()\n",
    "        }\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def apply_combo_freq_encoders(\n",
    "    df: pd.DataFrame,\n",
    "    combo_encoders: Dict[Tuple[str, str], Dict[Tuple[str, str], float]],\n",
    ") -> pd.DataFrame:\n",
    "    for (c1, c2), mapping in combo_encoders.items():\n",
    "        if c1 not in df.columns or c2 not in df.columns:\n",
    "            continue\n",
    "        col_name = f\"{c1}__{c2}__freq\"\n",
    "        s1 = _as_string_series(df[c1]).tolist()\n",
    "        s2 = _as_string_series(df[c2]).tolist()\n",
    "        df[col_name] = np.array(\n",
    "            [mapping.get(pair, 0.0) for pair in zip(s1, s2)],\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_target_encoders(\n",
    "    df: pd.DataFrame,\n",
    "    cols: List[str],\n",
    "    y_col: str,\n",
    "    alpha: float = 25.0,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    encoders: Dict[str, Dict[str, float]] = {}\n",
    "    if not cols:\n",
    "        return encoders\n",
    "    prior = float(df[y_col].mean())\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        temp = pd.DataFrame({col: _as_string_series(df[col]), y_col: df[y_col]})\n",
    "        stats = temp.groupby(col)[y_col].agg([\"mean\", \"count\"])\n",
    "        mapping: Dict[str, float] = {}\n",
    "        for value, row in stats.iterrows():\n",
    "            smooth = (row[\"mean\"] * row[\"count\"] + prior * alpha) / (row[\"count\"] + alpha)\n",
    "            mapping[value] = float(smooth)\n",
    "        mapping[\"__prior__\"] = prior\n",
    "        encoders[col] = mapping\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def apply_target_encoders(\n",
    "    df: pd.DataFrame,\n",
    "    target_encoders: Dict[str, Dict[str, float]],\n",
    ") -> pd.DataFrame:\n",
    "    for col, mapping in target_encoders.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        prior = mapping.get(\"__prior__\", 0.0)\n",
    "        df[f\"{col}__target_mean\"] = (\n",
    "            _as_string_series(df[col])\n",
    "            .map(mapping)\n",
    "            .fillna(prior)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    encoders: Dict[str, Dict[str, float]],\n",
    "    amt_col: Optional[str],\n",
    "    feat_cols: Optional[List[str]] = None,\n",
    "    *,\n",
    "    combo_encoders: Optional[Dict[Tuple[str, str], Dict[Tuple[str, str], float]]] = None,\n",
    "    target_encoders: Optional[Dict[str, Dict[str, float]]] = None,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    df = df.copy()\n",
    "\n",
    "    if CFG.DT_COL in df.columns:\n",
    "        df = add_time_features_pd(df, CFG.DT_COL)\n",
    "\n",
    "    if amt_col is not None and amt_col in df.columns:\n",
    "        df = add_amount_features_pd(df, amt_col)\n",
    "    else:\n",
    "        df[\"amt\"] = np.nan\n",
    "        df[\"amt_abs_log1p\"] = np.nan\n",
    "        df[\"amt_sign\"] = 0\n",
    "\n",
    "    if encoders:\n",
    "        df = apply_freq_encoders(df, encoders)\n",
    "    if combo_encoders:\n",
    "        df = apply_combo_freq_encoders(df, combo_encoders)\n",
    "    if target_encoders:\n",
    "        df = apply_target_encoders(df, target_encoders)\n",
    "\n",
    "    drop_cols = {CFG.CUSTOMER_COL, CFG.EVENT_ID_COL, CFG.DT_COL, CFG.TARGET_COL, CFG.Y_COL}\n",
    "    if feat_cols is None:\n",
    "        feat_cols = []\n",
    "        for c in df.columns:\n",
    "            if c in drop_cols:\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                feat_cols.append(c)\n",
    "    else:\n",
    "        for c in feat_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = np.nan\n",
    "\n",
    "    X = df[feat_cols].replace([np.inf, -np.inf], np.nan).fillna(-1.0)\n",
    "    return X, feat_cols\n",
    "\n",
    "\n",
    "def build_customer_activity_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CFG.CUSTOMER_COL not in df.columns or CFG.DT_COL not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    grouped = df.groupby(CFG.CUSTOMER_COL, observed=True)\n",
    "    stats = grouped.agg(\n",
    "        cust_total_ops=(CFG.EVENT_ID_COL, \"count\"),\n",
    "        cust_unique_event_types=(\"event_type_nm\", \"nunique\"),\n",
    "        cust_unique_channels=(\"channel_indicator_type\", \"nunique\"),\n",
    "        cust_first_time=(CFG.DT_COL, \"min\"),\n",
    "        cust_last_time=(CFG.DT_COL, \"max\"),\n",
    "    )\n",
    "    stats[\"cust_span_hours\"] = (\n",
    "        (stats[\"cust_last_time\"] - stats[\"cust_first_time\"]).dt.total_seconds().fillna(0.0) / 3600.0\n",
    "    )\n",
    "    stats[\"cust_span_hours\"] = stats[\"cust_span_hours\"].clip(lower=0.0)\n",
    "    stats[\"cust_ops_per_hour\"] = stats[\"cust_total_ops\"] / stats[\"cust_span_hours\"].replace(0.0, 1.0)\n",
    "    stats[\"cust_ops_per_hour\"] = stats[\"cust_ops_per_hour\"].fillna(0.0)\n",
    "    stats = stats.drop(columns=[\"cust_first_time\", \"cust_last_time\"])\n",
    "    return stats\n",
    "\n",
    "\n",
    "def apply_customer_activity_stats(df: pd.DataFrame, stats: pd.DataFrame) -> pd.DataFrame:\n",
    "    if stats.empty or CFG.CUSTOMER_COL not in df.columns:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for col in stats.columns:\n",
    "        out[col] = out[CFG.CUSTOMER_COL].map(stats[col]).fillna(0.0).astype(\"float32\")\n",
    "    out[\"cust_total_ops_log\"] = np.log1p(out[\"cust_total_ops\"]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_tqdm_callback(pbar: tqdm, postfix_every: int = 10):\n",
    "    def _cb(env):\n",
    "        pbar.update(1)\n",
    "        if env.iteration % postfix_every == 0 and env.evaluation_result_list:\n",
    "            valid_items = [x for x in env.evaluation_result_list if x[0] in (\"valid\", \"val\", \"valid_0\")]\n",
    "            item = valid_items[0] if valid_items else env.evaluation_result_list[0]\n",
    "            pbar.set_postfix({f\"{item[0]}_{item[1]}\": float(item[2])})\n",
    "    _cb.order = 0\n",
    "    return _cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe223e93",
   "metadata": {},
   "source": [
    "## 3) Input file checks and amount-column detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0e1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected amount column: operaton_amt\n"
     ]
    }
   ],
   "source": [
    "# Check that all required files are available.\n",
    "for name in CFG.PRETRAIN_FILES + CFG.TRAIN_FILES + [CFG.TRAIN_LABELS_FILE, CFG.PRETEST_FILE, CFG.TEST_FILE]:\n",
    "    must_exist(CFG.DATA_DIR / name)\n",
    "\n",
    "# Detect amount-like column from one TRAIN file schema.\n",
    "train0 = pl.scan_parquet(CFG.DATA_DIR / CFG.TRAIN_FILES[0])\n",
    "train0_schema = train0.collect_schema()\n",
    "amt_col = pick_amount_col(list(train0_schema.keys()))\n",
    "print(\"Detected amount column:\", amt_col)\n",
    "\n",
    "if amt_col is None:\n",
    "    print(\"WARNING: No amount-like column was detected. Continuing without amount-derived features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3995682",
   "metadata": {},
   "source": [
    "## 4) Optional pre-train customer aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9073a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached customer feats: outputs\\cust_feats_pretrain.parquet\n",
      "cust_feats_pl: (100000, 9)\n"
     ]
    }
   ],
   "source": [
    "def build_pretrain_customer_feats() -> Optional[pl.DataFrame]:\n",
    "    if not CFG.BUILD_PRETRAIN_CUSTOMER_FEATS:\n",
    "        return None\n",
    "\n",
    "    cache_fp = CFG.OUT_DIR / CFG.PRETRAIN_CUST_FEATS_CACHE\n",
    "    if cache_fp.exists():\n",
    "        print(\"Loading cached customer feats:\", cache_fp)\n",
    "        return pl.read_parquet(cache_fp)\n",
    "\n",
    "    print(\"Building customer feats from PRETRAIN (first run, will cache)...\")\n",
    "\n",
    "\n",
    "    base_cols = [CFG.CUSTOMER_COL]\n",
    "    if amt_col is not None:\n",
    "        base_cols.append(amt_col)\n",
    "\n",
    "    maybe_cols = [\"mcc_code\", \"channel_indicator_type\", \"event_type_nm\", \"currency_iso_cd\"]\n",
    "    use_cols = base_cols + maybe_cols\n",
    "\n",
    "    lfs = []\n",
    "    for name in tqdm(CFG.PRETRAIN_FILES, desc=\"Scanning pretrain parts\"):\n",
    "        lf = pl.scan_parquet(CFG.DATA_DIR / name)\n",
    "        lf = select_existing(lf, use_cols)\n",
    "        lfs.append(lf)\n",
    "\n",
    "    lf_all = pl.concat(lfs, how=\"vertical_relaxed\")\n",
    "\n",
    "    aggs = [pl.len().alias(\"pt_cnt\")]\n",
    "    if amt_col is not None:\n",
    "        aggs += [\n",
    "            pl.col(amt_col).cast(pl.Float64).mean().alias(\"pt_amt_mean\"),\n",
    "            pl.col(amt_col).cast(pl.Float64).std().alias(\"pt_amt_std\"),\n",
    "            pl.col(amt_col).cast(pl.Float64).max().alias(\"pt_amt_max\"),\n",
    "        ]\n",
    "\n",
    "\n",
    "    schema_all = lf_all.collect_schema()\n",
    "    for c in maybe_cols:\n",
    "        if c in schema_all:\n",
    "            aggs.append(pl.col(c).n_unique().alias(f\"pt_{c}_nuniq\"))\n",
    "\n",
    "    cust_feats = lf_all.group_by(CFG.CUSTOMER_COL).agg(aggs).collect(engine=\"streaming\")\n",
    "    cust_feats.write_parquet(cache_fp)\n",
    "    print(\"Cached customer feats to:\", cache_fp)\n",
    "\n",
    "    return cust_feats\n",
    "\n",
    "cust_feats_pl = build_pretrain_customer_feats()\n",
    "print(\"cust_feats_pl:\", None if cust_feats_pl is None else cust_feats_pl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b728d",
   "metadata": {},
   "source": [
    "## 5) Build sampled train frame with labels and customer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3d296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading TRAIN parts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:44<00:00, 14.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sampled shape: (1799067, 32)\n",
      "y counts: {0: 1747629, 1: 51438}\n",
      "pos rate: 0.028591486587214374\n"
     ]
    }
   ],
   "source": [
    "def load_train_with_labels_sample() -> pl.DataFrame:\n",
    "    labels_fp = CFG.DATA_DIR / CFG.TRAIN_LABELS_FILE\n",
    "    labels_lf = pl.scan_parquet(labels_fp).select([CFG.EVENT_ID_COL, CFG.TARGET_COL])\n",
    "\n",
    "\n",
    "    want_cols = [\n",
    "        CFG.CUSTOMER_COL, CFG.EVENT_ID_COL, CFG.DT_COL,\n",
    "        \"event_type_nm\", \"event_desc\",\n",
    "        \"channel_indicator_type\", \"channel_indicator_subtype\",\n",
    "        \"currency_iso_cd\", \"mcc_code\", \"pos_cd\",\n",
    "        \"accept_language\", \"browser_language\", \"timezone\",\n",
    "        \"session_id\", \"operating_system_type\", \"device_system_version\",\n",
    "        \"screen_size\", \"developer_tools\", \"phone_voip_call_state\",\n",
    "        \"web_rdp_connection\", \"compromised\", \"battery\",\n",
    "    ]\n",
    "    if amt_col is not None:\n",
    "        want_cols.append(amt_col)\n",
    "\n",
    "    parts: List[pl.DataFrame] = []\n",
    "    thr = int(CFG.GREEN_SAMPLE_FRAC * CFG.GREEN_MOD_BASE)\n",
    "\n",
    "    for name in tqdm(CFG.TRAIN_FILES, desc=\"Loading TRAIN parts\"):\n",
    "        fp = CFG.DATA_DIR / name\n",
    "        lf = pl.scan_parquet(fp)\n",
    "\n",
    "        if CFG.MAX_ROWS_PER_FILE is not None:\n",
    "            lf = lf.head(CFG.MAX_ROWS_PER_FILE)\n",
    "\n",
    "        lf = select_existing(lf, want_cols)\n",
    "        lf = ensure_datetime(lf, CFG.DT_COL)\n",
    "\n",
    "        lf = lf.join(labels_lf, on=CFG.EVENT_ID_COL, how=\"left\")\n",
    "\n",
    "        # Binary target: 1 for unconfirmed operations, 0 otherwise.\n",
    "        lf = lf.with_columns((pl.col(CFG.TARGET_COL) == 1).cast(pl.Int8).fill_null(0).alias(CFG.Y_COL))\n",
    "\n",
    "        is_labeled = pl.col(CFG.TARGET_COL).is_not_null()\n",
    "\n",
    "        # Keep all labeled events.\n",
    "        lf_labeled = lf.filter(is_labeled)\n",
    "\n",
    "\n",
    "\n",
    "        lf_green = (\n",
    "            lf.filter(~is_labeled)\n",
    "              .with_columns(pl.col(CFG.EVENT_ID_COL).cast(pl.Int64))\n",
    "              .filter((pl.col(CFG.EVENT_ID_COL) % CFG.GREEN_MOD_BASE) < thr)\n",
    "        )\n",
    "\n",
    "        lf_out = pl.concat([lf_labeled, lf_green], how=\"vertical_relaxed\")\n",
    "\n",
    "\n",
    "        if cust_feats_pl is not None:\n",
    "            lf_out = lf_out.join(cust_feats_pl.lazy(), on=CFG.CUSTOMER_COL, how=\"left\")\n",
    "\n",
    "        parts.append(lf_out.collect(engine=\"streaming\"))\n",
    "\n",
    "    return pl.concat(parts, how=\"vertical_relaxed\")\n",
    "\n",
    "train_pl = load_train_with_labels_sample()\n",
    "print(\"Train sampled shape:\", train_pl.shape)\n",
    "\n",
    "train_df = train_pl.to_pandas(use_pyarrow_extension_array=False)\n",
    "del train_pl\n",
    "\n",
    "\n",
    "train_df[CFG.DT_COL] = pd.to_datetime(train_df[CFG.DT_COL], errors=\"coerce\")\n",
    "\n",
    "print(\"y counts:\", train_df[CFG.Y_COL].value_counts(dropna=False).to_dict())\n",
    "print(\"pos rate:\", float(train_df[CFG.Y_COL].mean()))\n",
    "\n",
    "CUSTOMER_ACTIVITY_STATS = build_customer_activity_stats(train_df)\n",
    "train_df = apply_customer_activity_stats(train_df, CUSTOMER_ACTIVITY_STATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a14a",
   "metadata": {},
   "source": [
    "## 6) Time-based split, feature encoding, and blend search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14b822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train part: (1534614, 38) Valid part: (264453, 38)\n",
      "Pos rate train: 0.02895907374753521 Pos rate valid: 0.026458387690818406\n",
      "Detected categorical cols: 8\n",
      "Example: ['mcc_code', 'accept_language', 'browser_language', 'device_system_version', 'screen_size', 'developer_tools', 'compromised', 'battery']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight = 33.531491190567266\n",
      "\n",
      "--- Train split model: m0 (seed=42) ---\n",
      "m0: best_iter=282, valid_ap=0.248375\n",
      "\n",
      "--- Train split model: m1 (seed=73) ---\n",
      "m1: best_iter=331, valid_ap=0.247618\n",
      "\n",
      "--- Train split model: tuned_b (seed=73) ---\n",
      "tuned_b: best_iter=429, valid_ap=0.248640\n",
      "\n",
      "--- Train split model: tuned_d (seed=42) ---\n",
      "tuned_d: best_iter=380, valid_ap=0.252128\n",
      "\n",
      "--- Train split model: tuned_e (seed=73) ---\n",
      "tuned_e: best_iter=309, valid_ap=0.248978\n",
      "Best validation blend: kind=triple members=['tuned_d', 'tuned_e', 'tuned_b'] weights=[0.7, 0.2, 0.1] valid_ap=0.252645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refit full: tuned_d\n",
      "Refit full: tuned_e\n",
      "Refit full: tuned_b\n",
      "Final ensemble members: ['tuned_d', 'tuned_e', 'tuned_b']\n",
      "Final ensemble weights: [0.7000000000000001, 0.2, 0.09999999999999992]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "valid_from_ts = pd.Timestamp(CFG.VALID_FROM)\n",
    "is_valid = train_df[CFG.DT_COL] >= valid_from_ts\n",
    "\n",
    "train_part = train_df.loc[~is_valid].copy()\n",
    "valid_part = train_df.loc[is_valid].copy()\n",
    "\n",
    "print(\"Train part:\", train_part.shape, \"Valid part:\", valid_part.shape)\n",
    "print(\"Pos rate train:\", float(train_part[CFG.Y_COL].mean()), \"Pos rate valid:\", float(valid_part[CFG.Y_COL].mean()))\n",
    "\n",
    "exclude = {CFG.CUSTOMER_COL, CFG.EVENT_ID_COL, CFG.DT_COL, CFG.TARGET_COL, CFG.Y_COL}\n",
    "cat_cols = infer_cat_cols(train_part, exclude=exclude)\n",
    "print(\"Detected categorical cols:\", len(cat_cols))\n",
    "print(\"Example:\", cat_cols[:20])\n",
    "\n",
    "encoders = fit_freq_encoders(train_part, cat_cols, max_unique=150_000)\n",
    "combo_encoders_cv = fit_combo_freq_encoders(\n",
    "    train_part,\n",
    "    CFG.COMBO_FREQ_COL_PAIRS,\n",
    "    max_unique=CFG.COMBO_FREQ_MAX_UNIQUE,\n",
    ")\n",
    "target_encoders_cv = fit_target_encoders(\n",
    "    train_part,\n",
    "    CFG.TARGET_ENCODER_COLS,\n",
    "    CFG.Y_COL,\n",
    "    CFG.TARGET_ENCODER_ALPHA,\n",
    ")\n",
    "\n",
    "X_train, feat_cols = build_feature_matrix(\n",
    "    train_part,\n",
    "    encoders,\n",
    "    amt_col=amt_col,\n",
    "    combo_encoders=combo_encoders_cv,\n",
    "    target_encoders=target_encoders_cv,\n",
    ")\n",
    "y_train = train_part[CFG.Y_COL].astype(\"int8\").to_numpy()\n",
    "\n",
    "X_valid, _ = build_feature_matrix(\n",
    "    valid_part,\n",
    "    encoders,\n",
    "    amt_col=amt_col,\n",
    "    feat_cols=feat_cols,\n",
    "    combo_encoders=combo_encoders_cv,\n",
    "    target_encoders=target_encoders_cv,\n",
    ")\n",
    "y_valid = valid_part[CFG.Y_COL].astype(\"int8\").to_numpy()\n",
    "\n",
    "pos = float(np.sum(y_train == 1))\n",
    "neg = float(np.sum(y_train == 0))\n",
    "scale_pos_weight = neg / max(pos, 1.0)\n",
    "print(\"scale_pos_weight =\", scale_pos_weight)\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    {\n",
    "        \"name\": \"m0\",\n",
    "        \"seed\": 42,\n",
    "        \"params\": {\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"num_leaves\": 64,\n",
    "            \"min_data_in_leaf\": 200,\n",
    "            \"feature_fraction\": 0.8,\n",
    "            \"bagging_fraction\": 0.8,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"lambda_l2\": 1.0,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"m1\",\n",
    "        \"seed\": 73,\n",
    "        \"params\": {\n",
    "            \"learning_rate\": 0.04,\n",
    "            \"num_leaves\": 96,\n",
    "            \"min_data_in_leaf\": 150,\n",
    "            \"feature_fraction\": 0.85,\n",
    "            \"bagging_fraction\": 0.8,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"lambda_l2\": 2.0,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tuned_b\",\n",
    "        \"seed\": 73,\n",
    "        \"params\": {\n",
    "            \"learning_rate\": 0.03,\n",
    "            \"num_leaves\": 128,\n",
    "            \"min_data_in_leaf\": 120,\n",
    "            \"feature_fraction\": 0.9,\n",
    "            \"bagging_fraction\": 0.85,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"lambda_l2\": 2.0,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tuned_d\",\n",
    "        \"seed\": 42,\n",
    "        \"params\": {\n",
    "            \"learning_rate\": 0.04,\n",
    "            \"num_leaves\": 80,\n",
    "            \"min_data_in_leaf\": 300,\n",
    "            \"feature_fraction\": 0.8,\n",
    "            \"bagging_fraction\": 0.8,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"lambda_l2\": 3.0,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tuned_e\",\n",
    "        \"seed\": 73,\n",
    "        \"params\": {\n",
    "            \"learning_rate\": 0.045,\n",
    "            \"num_leaves\": 96,\n",
    "            \"min_data_in_leaf\": 220,\n",
    "            \"feature_fraction\": 0.88,\n",
    "            \"bagging_fraction\": 0.9,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"lambda_l2\": 1.7,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "split_models = []\n",
    "for spec in MODEL_SPECS:\n",
    "    print(f\"\\n--- Train split model: {spec['name']} (seed={spec['seed']}) ---\")\n",
    "\n",
    "    params_local = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": [\"average_precision\"],\n",
    "        \"max_bin\": 255,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": spec[\"seed\"],\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"num_threads\": os.cpu_count() or 4,\n",
    "    }\n",
    "    params_local.update(spec[\"params\"])\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, feature_name=feat_cols, free_raw_data=True)\n",
    "    lgb_valid = lgb.Dataset(X_valid, label=y_valid, feature_name=feat_cols, reference=lgb_train, free_raw_data=True)\n",
    "\n",
    "    model_local = lgb.train(\n",
    "        params=params_local,\n",
    "        train_set=lgb_train,\n",
    "        num_boost_round=CFG.NUM_BOOST_ROUND,\n",
    "        valid_sets=[lgb_valid],\n",
    "        valid_names=[\"valid\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(CFG.EARLY_STOPPING, verbose=False),\n",
    "            lgb.log_evaluation(period=0),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    best_iter_local = int(model_local.best_iteration or CFG.NUM_BOOST_ROUND)\n",
    "    valid_pred_local = model_local.predict(X_valid, num_iteration=best_iter_local)\n",
    "    valid_ap_local = float(average_precision_score(y_valid, valid_pred_local))\n",
    "\n",
    "    print(f\"{spec['name']}: best_iter={best_iter_local}, valid_ap={valid_ap_local:.6f}\")\n",
    "\n",
    "    split_models.append(\n",
    "        {\n",
    "            \"name\": spec[\"name\"],\n",
    "            \"seed\": spec[\"seed\"],\n",
    "            \"params\": params_local,\n",
    "            \"best_iter\": best_iter_local,\n",
    "            \"valid_ap\": valid_ap_local,\n",
    "            \"valid_pred\": valid_pred_local,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Search best pair blend on validation\n",
    "best_blend = {\n",
    "    \"ap\": -1.0,\n",
    "    \"kind\": None,\n",
    "    \"members\": [],\n",
    "    \"weights\": [],\n",
    "}\n",
    "name_to_pred = {m[\"name\"]: m[\"valid_pred\"] for m in split_models}\n",
    "model_names = [m[\"name\"] for m in split_models]\n",
    "\n",
    "for a, b in combinations(model_names, 2):\n",
    "    p_a = name_to_pred[a]\n",
    "    p_b = name_to_pred[b]\n",
    "    for w in np.linspace(0.0, 1.0, 41):\n",
    "        p_bl = (1.0 - w) * p_a + w * p_b\n",
    "        ap_bl = float(average_precision_score(y_valid, p_bl))\n",
    "        if ap_bl > best_blend[\"ap\"]:\n",
    "            best_blend = {\n",
    "                \"ap\": ap_bl,\n",
    "                \"kind\": \"pair\",\n",
    "                \"members\": [a, b],\n",
    "                \"weights\": [1.0 - float(w), float(w)],\n",
    "            }\n",
    "\n",
    "# Optional top-3 triple blend (coarse simplex)\n",
    "top3 = sorted(split_models, key=lambda d: d[\"valid_ap\"], reverse=True)[:3]\n",
    "if len(top3) == 3:\n",
    "    n0, n1, n2 = [x[\"name\"] for x in top3]\n",
    "    p0, p1, p2 = [name_to_pred[n0], name_to_pred[n1], name_to_pred[n2]]\n",
    "    grid = np.linspace(0.0, 1.0, 11)\n",
    "    for w0 in grid:\n",
    "        for w1 in grid:\n",
    "            w2 = 1.0 - w0 - w1\n",
    "            if w2 < 0:\n",
    "                continue\n",
    "            p_bl = w0 * p0 + w1 * p1 + w2 * p2\n",
    "            ap_bl = float(average_precision_score(y_valid, p_bl))\n",
    "            if ap_bl > best_blend[\"ap\"]:\n",
    "                best_blend = {\n",
    "                    \"ap\": ap_bl,\n",
    "                    \"kind\": \"triple\",\n",
    "                    \"members\": [n0, n1, n2],\n",
    "                    \"weights\": [float(w0), float(w1), float(w2)],\n",
    "                }\n",
    "\n",
    "print(\n",
    "    \"Best validation blend:\",\n",
    "    f\"kind={best_blend['kind']}\",\n",
    "    f\"members={best_blend['members']}\",\n",
    "    f\"weights={[round(w, 4) for w in best_blend['weights']]}\",\n",
    "    f\"valid_ap={best_blend['ap']:.6f}\",\n",
    ")\n",
    "\n",
    "combo_encoders_full = fit_combo_freq_encoders(\n",
    "    train_df,\n",
    "    CFG.COMBO_FREQ_COL_PAIRS,\n",
    "    max_unique=CFG.COMBO_FREQ_MAX_UNIQUE,\n",
    ")\n",
    "target_encoders_full = fit_target_encoders(\n",
    "    train_df,\n",
    "    CFG.TARGET_ENCODER_COLS,\n",
    "    CFG.Y_COL,\n",
    "    CFG.TARGET_ENCODER_ALPHA,\n",
    ")\n",
    "\n",
    "# Refit selected models on full sampled train\n",
    "name_to_split = {m[\"name\"]: m for m in split_models}\n",
    "\n",
    "X_full, _ = build_feature_matrix(\n",
    "    train_df,\n",
    "    encoders,\n",
    "    amt_col=amt_col,\n",
    "    feat_cols=feat_cols,\n",
    "    combo_encoders=combo_encoders_full,\n",
    "    target_encoders=target_encoders_full,\n",
    ")\n",
    "y_full = train_df[CFG.Y_COL].astype(\"int8\").to_numpy()\n",
    "\n",
    "pos_full = float(np.sum(y_full == 1))\n",
    "neg_full = float(np.sum(y_full == 0))\n",
    "scale_pos_weight_full = neg_full / max(pos_full, 1.0)\n",
    "\n",
    "ENSEMBLE_MODELS = []\n",
    "for name in best_blend[\"members\"]:\n",
    "    sm = name_to_split[name]\n",
    "    print(f\"Refit full: {name}\")\n",
    "\n",
    "    params_full = dict(sm[\"params\"])\n",
    "    params_full[\"scale_pos_weight\"] = scale_pos_weight_full\n",
    "\n",
    "    lgb_full = lgb.Dataset(X_full, label=y_full, feature_name=feat_cols, free_raw_data=True)\n",
    "    model_full = lgb.train(\n",
    "        params=params_full,\n",
    "        train_set=lgb_full,\n",
    "        num_boost_round=sm[\"best_iter\"],\n",
    "        valid_sets=[],\n",
    "        callbacks=[lgb.log_evaluation(period=0)],\n",
    "    )\n",
    "\n",
    "    ENSEMBLE_MODELS.append(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"model\": model_full,\n",
    "            \"best_iter\": sm[\"best_iter\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "ENSEMBLE_WEIGHTS = np.asarray(best_blend[\"weights\"], dtype=\"float64\")\n",
    "print(\"Final ensemble members:\", [m[\"name\"] for m in ENSEMBLE_MODELS])\n",
    "print(\"Final ensemble weights:\", ENSEMBLE_WEIGHTS.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f12ab",
   "metadata": {},
   "source": [
    "## 7) Fit final ensemble and generate test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49964aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned_d pred stats: 0.001160679798217419 0.29632596341369616 0.9912954566434496\n",
      "tuned_e pred stats: 0.0010949394444196604 0.2913249663682736 0.9912875647342297\n",
      "tuned_b pred stats: 0.0009310635691051099 0.2844309022556963 0.9897430167280525\n",
      "Submission saved to: D:\\Apps\\PyCharm\\pythonProject\\fusion_ipynb\\outputs\\8.csv\n",
      "Rows: 633683\n"
     ]
    }
   ],
   "source": [
    "test_fp = CFG.DATA_DIR / CFG.TEST_FILE\n",
    "test_pl = pl.read_parquet(test_fp)\n",
    "\n",
    "if cust_feats_pl is not None:\n",
    "    test_pl = test_pl.join(cust_feats_pl, on=CFG.CUSTOMER_COL, how=\"left\")\n",
    "\n",
    "test_df = test_pl.to_pandas(use_pyarrow_extension_array=False)\n",
    "del test_pl\n",
    "\n",
    "if CFG.DT_COL in test_df.columns:\n",
    "    test_df[CFG.DT_COL] = pd.to_datetime(test_df[CFG.DT_COL], errors=\"coerce\")\n",
    "\n",
    "test_df = apply_customer_activity_stats(test_df, CUSTOMER_ACTIVITY_STATS)\n",
    "\n",
    "X_test, _ = build_feature_matrix(\n",
    "    test_df,\n",
    "    encoders,\n",
    "    amt_col=amt_col,\n",
    "    feat_cols=feat_cols,\n",
    "    combo_encoders=combo_encoders_full,\n",
    "    target_encoders=target_encoders_full,\n",
    ")\n",
    "\n",
    "pred_parts = []\n",
    "for m in ENSEMBLE_MODELS:\n",
    "    p_m = m[\"model\"].predict(X_test, num_iteration=m[\"best_iter\"])\n",
    "    pred_parts.append(p_m)\n",
    "    print(f\"{m['name']} pred stats:\", float(np.min(p_m)), float(np.mean(p_m)), float(np.max(p_m)))\n",
    "\n",
    "pred = np.zeros(len(test_df), dtype=\"float64\")\n",
    "for w, p_m in zip(ENSEMBLE_WEIGHTS, pred_parts):\n",
    "    pred += float(w) * p_m\n",
    "\n",
    "submit_path = CFG.OUT_DIR / \"8.csv\"\n",
    "out = pd.DataFrame({CFG.EVENT_ID_COL: test_df[CFG.EVENT_ID_COL].values, \"predict\": pred})\n",
    "out.to_csv(submit_path, index=False)\n",
    "\n",
    "print(\"Submission saved to:\", submit_path.resolve())\n",
    "print(\"Rows:\", len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0866175",
   "metadata": {},
   "source": [
    "## 8) Additional honest-ish validation snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1ce0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honest-ish AP (tuned_d): 0.29277600301119544\n",
      "honest-ish AP (tuned_e): 0.3009941490866072\n",
      "honest-ish AP (tuned_b): 0.3220252269541291\n",
      "honest-ish valid pos_rate = 0.01680637955468018\n",
      "honest-ish PR-AUC (AP)    = 0.3009431598066904\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "VALID_FROM = CFG.VALID_FROM\n",
    "VALID_TO = \"2025-05-31\"\n",
    "GREEN_PER_LABELED = 30\n",
    "\n",
    "labels_lf = (\n",
    "    pl.scan_parquet(CFG.DATA_DIR / CFG.TRAIN_LABELS_FILE)\n",
    "      .select([CFG.EVENT_ID_COL, CFG.TARGET_COL])\n",
    ")\n",
    "\n",
    "valid_parts = []\n",
    "for name in CFG.TRAIN_FILES:\n",
    "    ops = pl.scan_parquet(CFG.DATA_DIR / name)\n",
    "    ops = ensure_datetime(ops, CFG.DT_COL)\n",
    "\n",
    "    ops_win = ops.filter(\n",
    "        (pl.col(CFG.DT_COL) >= pl.datetime(*map(int, VALID_FROM.split(\"-\")), 0, 0, 0))\n",
    "        & (pl.col(CFG.DT_COL) <= pl.datetime(*map(int, VALID_TO.split(\"-\")), 23, 59, 59))\n",
    "    )\n",
    "\n",
    "    joined = ops_win.join(labels_lf, on=CFG.EVENT_ID_COL, how=\"left\")\n",
    "\n",
    "    labeled = joined.filter(pl.col(CFG.TARGET_COL).is_not_null()).with_columns(\n",
    "        (pl.col(CFG.TARGET_COL) == 1).cast(pl.Int8).alias(\"y\")\n",
    "    )\n",
    "\n",
    "    green = joined.filter(pl.col(CFG.TARGET_COL).is_null()).with_columns(\n",
    "        pl.lit(0).cast(pl.Int8).alias(\"y\")\n",
    "    )\n",
    "\n",
    "    labeled_df = labeled.collect(engine=\"streaming\")\n",
    "    green_df = green.collect(engine=\"streaming\")\n",
    "    want_green = int(labeled_df.height * GREEN_PER_LABELED)\n",
    "\n",
    "    green_s = (\n",
    "        green_df.sample(n=min(want_green, green_df.height), seed=CFG.SEED)\n",
    "        if want_green > 0\n",
    "        else green_df.head(0)\n",
    "    )\n",
    "\n",
    "    part = pl.concat([labeled_df, green_s], how=\"vertical_relaxed\")\n",
    "    if cust_feats_pl is not None:\n",
    "        part = part.join(cust_feats_pl, on=CFG.CUSTOMER_COL, how=\"left\")\n",
    "\n",
    "    valid_parts.append(part)\n",
    "\n",
    "valid_df = pl.concat(valid_parts, how=\"vertical_relaxed\").to_pandas(use_pyarrow_extension_array=False)\n",
    "\n",
    "valid_df[CFG.DT_COL] = pd.to_datetime(valid_df[CFG.DT_COL], errors=\"coerce\")\n",
    "y_valid_honest = valid_df[\"y\"].astype(\"int8\").to_numpy()\n",
    "\n",
    "valid_df = apply_customer_activity_stats(valid_df, CUSTOMER_ACTIVITY_STATS)\n",
    "\n",
    "X_valid_honest, _ = build_feature_matrix(\n",
    "    valid_df,\n",
    "    encoders,\n",
    "    amt_col=amt_col,\n",
    "    feat_cols=feat_cols,\n",
    "    combo_encoders=combo_encoders_cv,\n",
    "    target_encoders=target_encoders_cv,\n",
    ")\n",
    "\n",
    "pred_parts = []\n",
    "for m in ENSEMBLE_MODELS:\n",
    "    p_m = m[\"model\"].predict(X_valid_honest, num_iteration=m[\"best_iter\"])\n",
    "    ap_m = float(average_precision_score(y_valid_honest, p_m))\n",
    "    print(f\"honest-ish AP ({m['name']}):\", ap_m)\n",
    "    pred_parts.append(p_m)\n",
    "\n",
    "p_valid_honest = np.zeros(len(valid_df), dtype=\"float64\")\n",
    "for w, p_m in zip(ENSEMBLE_WEIGHTS, pred_parts):\n",
    "    p_valid_honest += float(w) * p_m\n",
    "\n",
    "print(\"honest-ish valid pos_rate =\", float(y_valid_honest.mean()))\n",
    "print(\"honest-ish PR-AUC (AP)    =\", float(average_precision_score(y_valid_honest, p_valid_honest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e976066f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFinal PR-AUC on public ds = 0,1398083541\\nAt the time of submitting the solution it was 2nd place on leaderboard\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Final PR-AUC on public ds = 0,1398083541\n",
    "At the time of submitting the solution it was 2nd place on leaderboard\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
