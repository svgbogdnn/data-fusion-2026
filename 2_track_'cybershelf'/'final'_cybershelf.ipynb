{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70adcdad",
   "metadata": {
    "papermill": {
     "duration": 0.00347,
     "end_time": "2026-02-26T17:05:46.481104",
     "exception": false,
     "start_time": "2026-02-26T17:05:46.477634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Fusion Contest 2026 - Задача 2 \"Киберполка\"\n",
    "\n",
    "Участникам необходимо решить задачу multi-label классификации для 41 финансового продукта клиентов банка на основе обезличенных данных заранее предоставленных признаков.\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "Клиенты банка владеют разными банковскими продуктами — различными видами счетов, карт и услуг. Одним из запросов бизнеса является желание ранжировать эти продукты по вероятности их открытия клиентами.\n",
    "\n",
    "В данной задаче продуктов “на полке” 41 штука. В отличие от классических задач рекомендаций, бизнес интересуют вероятности открытия каждого из продуктов. Умея хорошо предсказывать эти вероятности, бизнес имел бы возможность гибко настраивать рекомендации.\n",
    "\n",
    "В этом соревновании участникам предстоит работать с полностью анонимными и обфусцированными данными 1,000,0000 клиентов:\n",
    "\n",
    "- Описания банковских продуктов не передаются. Также не передаются и описания признаков. Для признаков передается только информация об их исходном типе (категорийные признаки cat_feature_i и числовые признаки num_feature_j).\n",
    "- Признаков достаточно много: основной набор в 200 признаков, а также дополнительный набор с более 2,000 признаками.\n",
    "- В признаках много пропущенных значений, а также присутствуют выбросы.\n",
    "- 750,000 клиентов имеют разметку и составляют тренировочные данные. Оставшиеся 250,000 клиентов составляют тестовые данные.\n",
    "\n",
    "Подробнее про структуру данных можно узнать на странице “Данные”.\n",
    "\n",
    "## Формат решений\n",
    "\n",
    "Это соревнование с разметкой предоставленного вам .parquet файла. Вам необходимо создать алгоритм, способный по предоставленным в рамках соревнования данным, создать новый .parquet файл с 42 столбцами:\n",
    "\n",
    "```text\n",
    "customer_id, predict_1_1, predict_1_2, ... , predict_10_1\n",
    "1750000, -4.921889, -5.700829, ... , -0.954659\n",
    "1750001, -4.963202, -6.826517, ... , -0.622011\n",
    "...\n",
    "1999999, -4.249957, -4.785856, ... , -0.931220\n",
    "```\n",
    "\n",
    "- customer_id – идентификатор клиента;\n",
    "- predict_i – предсказание вашего алгоритма для класса target_i. Например predict_1_1 для target_1_1 и т.д.\n",
    "\n",
    "Предсказания необходимо построить для всех 250,000 клиентов в тестовых данных.\n",
    "\n",
    "Пример sample_submit.parquet доступен на странице “Данные”.\n",
    "\n",
    "## Проверка решений\n",
    "\n",
    "Решения проверяются автоматически путем сопоставления с известными истинными историческими значениями суммарных переводов клиентов банка со своих счетов. Истинные исторические значения в тестовых данных доступны только организаторам.\n",
    "\n",
    "Метрика соревнования — Macro Averaged ROC-AUC. Для multi-label это эквивалентно простому усреднению ROC-AUC по каждому классу.\n",
    "\n",
    "Для расчета метрики используется sklearn имплементация:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "```\n",
    "\n",
    "Соотношение public/private в соревновании составляет 30/70:\n",
    "\n",
    "- 75,000 (30%) клиентских записей используются для результатов на public лидерборде.\n",
    "- 175,000 (70%) клиентских записей  используются для результатов на private лидерборде.\n",
    "\n",
    "Победители соревнования определяются по результатам на private лидерборде.\n",
    "Для private лидерборда можно выбрать до 2 финальных решений.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98f715",
   "metadata": {
    "papermill": {
     "duration": 0.002502,
     "end_time": "2026-02-26T17:05:46.486236",
     "exception": false,
     "start_time": "2026-02-26T17:05:46.483734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Данные\n",
    "\n",
    "Для решения задачи 2 “Киберполка” предлагается два наборов данных, а также пример решения задачи.\n",
    "\n",
    "**Важные напоминания:**\n",
    "\n",
    "- Расшифровка названий признаков, как и расшифровка названий целевых переменных, не предоставляется.\n",
    "- Решения принимаются в формате .parquet. Форма файлов, названия и порядок столбцов должны строго соответствовать таковым в sample_submit.parquet.\n",
    "\n",
    "### Основные материалы соревнования\n",
    "\n",
    "- `train_target.parquet` ? 4.8MB, разметка целевых переменных для тренировочных данных\n",
    "- `sample_submit.parquet` ? 43.9MB, пример базового решения на основе Catboost\n",
    "- `baseline_catboost.ipynb` ? 231.5KB, ноутбук с реализацией базового решения на основе Catboost\n",
    "\n",
    "### Тренировочные данные\n",
    "\n",
    "Для обучения моделей участникам представляются 2 файла с различными группами признаков для моделирования.\n",
    "В качестве ключа для объединения данных выступает идентификатор клиента customer_id.\n",
    "\n",
    "- `train_main_features.parquet` ? 119.0MB, основная группа признаков (категорийные и числовые)\n",
    "- `train_extra_features.parquet` ? 959.3MB, дополнительные признаки для решения задачи (числовые признаки)\n",
    "\n",
    "### Тестовые данные\n",
    "\n",
    "Аналогичные 2 файла, необходимые для подготовки ваших предсказаний:\n",
    "\n",
    "- `test_main_features.parquet` ? 42.4MB, основная группа признаков (категорийные и числовые)\n",
    "- `test_extra_features.parquet` ? 320.4MB, дополнительные признаки для решения задачи (числовые признаки)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2bab4",
   "metadata": {
    "papermill": {
     "duration": 0.00238,
     "end_time": "2026-02-26T17:05:46.491012",
     "exception": false,
     "start_time": "2026-02-26T17:05:46.488632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Imports and utility functions\n",
    "\n",
    "This section initializes libraries, memory-safe helpers, and streaming DSV builders for Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b922ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:05:46.497632Z",
     "iopub.status.busy": "2026-02-26T17:05:46.497331Z",
     "iopub.status.idle": "2026-02-26T17:05:51.076413Z",
     "shell.execute_reply": "2026-02-26T17:05:51.075735Z"
    },
    "papermill": {
     "duration": 4.584715,
     "end_time": "2026-02-26T17:05:51.078142",
     "exception": false,
     "start_time": "2026-02-26T17:05:46.493427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "DATA_DIR = Path('/kaggle/input/datasets/yngbogdnn27/fusion2')\n",
    "WORK_DIR = Path('/kaggle/working/catboost_cache')\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_DSV = WORK_DIR / 'train_full.dsv'\n",
    "TEST_DSV = WORK_DIR / 'test_full.dsv'\n",
    "TRAIN_CD = WORK_DIR / 'train.cd'\n",
    "TEST_CD = WORK_DIR / 'test.cd'\n",
    "\n",
    "BATCH_ROWS = 1000  # smaller value => lower RAM peak, slower preprocessing\n",
    "\n",
    "\n",
    "def write_cd(cd_path: Path, label_count: int, feature_cols, cat_feature_names):\n",
    "    lines = []\n",
    "\n",
    "    for i in range(label_count):\n",
    "        lines.append(f'{i}\\tLabel\\n')\n",
    "\n",
    "    feature_offset = label_count\n",
    "    cat_set = set(cat_feature_names)\n",
    "    for j, feature_name in enumerate(feature_cols):\n",
    "        if feature_name in cat_set:\n",
    "            col_idx = feature_offset + j\n",
    "            lines.append(f'{col_idx}\\tCateg\\t{feature_name}\\n')\n",
    "\n",
    "    cd_path.write_text(''.join(lines), encoding='utf-8')\n",
    "\n",
    "\n",
    "def _safe_zip(*iterables):\n",
    "    sentinel = object()\n",
    "    for items in zip_longest(*iterables, fillvalue=sentinel):\n",
    "        if any(x is sentinel for x in items):\n",
    "            raise ValueError('Batch streams are misaligned (different number of batches).')\n",
    "        yield items\n",
    "\n",
    "\n",
    "def _append_batch_to_dsv(df: pd.DataFrame, path: Path):\n",
    "    df.to_csv(\n",
    "        path,\n",
    "        sep='\\t',\n",
    "        header=False,\n",
    "        index=False,\n",
    "        mode='a',\n",
    "        na_rep='nan',\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        float_format='%.7g',\n",
    "    )\n",
    "\n",
    "\n",
    "def build_catboost_dsv(\n",
    "    main_path: Path,\n",
    "    extra_path: Path,\n",
    "    output_path: Path,\n",
    "    feature_main_cols,\n",
    "    extra_cols_to_add,\n",
    "    cat_feature_names,\n",
    "    key: str = 'customer_id',\n",
    "    batch_rows: int = BATCH_ROWS,\n",
    "    target_path: Path = None,\n",
    "    target_cols=None,\n",
    "):\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()\n",
    "\n",
    "    main_schema_cols = pq.ParquetFile(main_path).schema.names\n",
    "    extra_schema_cols = pq.ParquetFile(extra_path).schema.names\n",
    "\n",
    "    main_scan_cols = list(feature_main_cols)\n",
    "    extra_scan_cols = list(extra_cols_to_add)\n",
    "\n",
    "    check_key = key in main_schema_cols and key in extra_schema_cols\n",
    "    if check_key:\n",
    "        if key not in main_scan_cols:\n",
    "            main_scan_cols.append(key)\n",
    "        if key not in extra_scan_cols:\n",
    "            extra_scan_cols.append(key)\n",
    "\n",
    "    main_batches = ds.dataset(main_path, format='parquet').scanner(\n",
    "        columns=main_scan_cols,\n",
    "        batch_size=batch_rows,\n",
    "        use_threads=True,\n",
    "    ).to_batches()\n",
    "\n",
    "    extra_batches = ds.dataset(extra_path, format='parquet').scanner(\n",
    "        columns=extra_scan_cols,\n",
    "        batch_size=batch_rows,\n",
    "        use_threads=True,\n",
    "    ).to_batches()\n",
    "\n",
    "    if target_path is not None:\n",
    "        if not target_cols:\n",
    "            raise ValueError('target_cols must be provided when target_path is set.')\n",
    "\n",
    "        target_batches = ds.dataset(target_path, format='parquet').scanner(\n",
    "            columns=list(target_cols),\n",
    "            batch_size=batch_rows,\n",
    "            use_threads=True,\n",
    "        ).to_batches()\n",
    "\n",
    "        iterator = _safe_zip(main_batches, extra_batches, target_batches)\n",
    "    else:\n",
    "        iterator = _safe_zip(main_batches, extra_batches)\n",
    "\n",
    "    for batch_idx, packed in enumerate(iterator, start=1):\n",
    "        if target_path is not None:\n",
    "            main_batch, extra_batch, target_batch = packed\n",
    "        else:\n",
    "            main_batch, extra_batch = packed\n",
    "            target_batch = None\n",
    "\n",
    "        main_df = main_batch.to_pandas()\n",
    "        extra_df = extra_batch.to_pandas()\n",
    "\n",
    "        if check_key:\n",
    "            if not main_df[key].equals(extra_df[key]):\n",
    "                raise ValueError(f'Rows are misaligned by {key} in batch {batch_idx}.')\n",
    "\n",
    "        if key in main_df.columns:\n",
    "            main_df.drop(columns=[key], inplace=True)\n",
    "        if key in extra_df.columns:\n",
    "            extra_df.drop(columns=[key], inplace=True)\n",
    "\n",
    "        feature_df = pd.concat(\n",
    "            [main_df[feature_main_cols], extra_df[extra_cols_to_add]],\n",
    "            axis=1,\n",
    "            copy=False,\n",
    "        )\n",
    "\n",
    "        if target_batch is not None:\n",
    "            target_df = target_batch.to_pandas()\n",
    "            chunk_df = pd.concat([target_df[list(target_cols)], feature_df], axis=1, copy=False)\n",
    "            del target_df\n",
    "        else:\n",
    "            chunk_df = feature_df\n",
    "\n",
    "        _append_batch_to_dsv(chunk_df, output_path)\n",
    "\n",
    "        del main_df, extra_df, feature_df, chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Processed {batch_idx} batches -> {output_path.name}')\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596d58e",
   "metadata": {
    "papermill": {
     "duration": 0.002445,
     "end_time": "2026-02-26T17:05:51.083246",
     "exception": false,
     "start_time": "2026-02-26T17:05:51.080801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Dataset paths and schema checks\n",
    "\n",
    "Define all input paths, infer feature/target schema, and validate train/test consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57471a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:05:51.089426Z",
     "iopub.status.busy": "2026-02-26T17:05:51.089078Z",
     "iopub.status.idle": "2026-02-26T17:05:51.253571Z",
     "shell.execute_reply": "2026-02-26T17:05:51.252700Z"
    },
    "papermill": {
     "duration": 0.169351,
     "end_time": "2026-02-26T17:05:51.255150",
     "exception": false,
     "start_time": "2026-02-26T17:05:51.085799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target columns: 41\n",
      "Feature columns: 2440\n",
      "Categorical features: 67\n"
     ]
    }
   ],
   "source": [
    "train_main_path = DATA_DIR / 'train_main_features.parquet'\n",
    "train_extra_path = DATA_DIR / 'train_extra_features.parquet'\n",
    "test_main_path = DATA_DIR / 'test_main_features.parquet'\n",
    "test_extra_path = DATA_DIR / 'test_extra_features.parquet'\n",
    "target_path = DATA_DIR / 'train_target.parquet'\n",
    "\n",
    "train_main_cols = pq.ParquetFile(train_main_path).schema.names\n",
    "train_extra_cols = pq.ParquetFile(train_extra_path).schema.names\n",
    "test_main_cols = pq.ParquetFile(test_main_path).schema.names\n",
    "test_extra_cols = pq.ParquetFile(test_extra_path).schema.names\n",
    "target_cols = [c for c in pq.ParquetFile(target_path).schema.names if c != 'customer_id']\n",
    "\n",
    "feature_main_cols = [c for c in train_main_cols if c != 'customer_id']\n",
    "extra_cols_to_add = [c for c in train_extra_cols if c not in train_main_cols and c != 'customer_id']\n",
    "feature_cols = feature_main_cols + extra_cols_to_add\n",
    "cat_feature_names = [c for c in feature_cols if c.startswith('cat_feature')]\n",
    "\n",
    "# Keep train/test feature layout identical.\n",
    "test_feature_main_cols = [c for c in test_main_cols if c != 'customer_id']\n",
    "test_extra_cols_to_add = [c for c in test_extra_cols if c not in test_main_cols and c != 'customer_id']\n",
    "test_feature_cols = test_feature_main_cols + test_extra_cols_to_add\n",
    "\n",
    "if feature_cols != test_feature_cols:\n",
    "    raise ValueError('Train/Test feature layouts differ. Cannot build a consistent pool.')\n",
    "\n",
    "write_cd(TRAIN_CD, label_count=len(target_cols), feature_cols=feature_cols, cat_feature_names=cat_feature_names)\n",
    "write_cd(TEST_CD, label_count=0, feature_cols=feature_cols, cat_feature_names=cat_feature_names)\n",
    "\n",
    "print('Target columns:', len(target_cols))\n",
    "print('Feature columns:', len(feature_cols))\n",
    "print('Categorical features:', len(cat_feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0624a",
   "metadata": {
    "papermill": {
     "duration": 0.002559,
     "end_time": "2026-02-26T17:05:51.260469",
     "exception": false,
     "start_time": "2026-02-26T17:05:51.257910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Build training artifacts (streaming)\n",
    "\n",
    "Create a memory-safe training DSV file from Parquet sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a58d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:05:51.267073Z",
     "iopub.status.busy": "2026-02-26T17:05:51.266468Z",
     "iopub.status.idle": "2026-02-26T17:34:42.039387Z",
     "shell.execute_reply": "2026-02-26T17:34:42.038632Z"
    },
    "papermill": {
     "duration": 1730.778062,
     "end_time": "2026-02-26T17:34:42.040994",
     "exception": false,
     "start_time": "2026-02-26T17:05:51.262932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 batches -> train_full.dsv\n",
      "Processed 40 batches -> train_full.dsv\n",
      "Processed 60 batches -> train_full.dsv\n",
      "Processed 80 batches -> train_full.dsv\n",
      "Processed 100 batches -> train_full.dsv\n",
      "Processed 120 batches -> train_full.dsv\n",
      "Processed 140 batches -> train_full.dsv\n",
      "Processed 160 batches -> train_full.dsv\n",
      "Processed 180 batches -> train_full.dsv\n",
      "Processed 200 batches -> train_full.dsv\n",
      "Processed 220 batches -> train_full.dsv\n",
      "Processed 240 batches -> train_full.dsv\n",
      "Processed 260 batches -> train_full.dsv\n",
      "Processed 280 batches -> train_full.dsv\n",
      "Processed 300 batches -> train_full.dsv\n",
      "Processed 320 batches -> train_full.dsv\n",
      "Processed 340 batches -> train_full.dsv\n",
      "Processed 360 batches -> train_full.dsv\n",
      "Processed 380 batches -> train_full.dsv\n",
      "Processed 400 batches -> train_full.dsv\n",
      "Processed 420 batches -> train_full.dsv\n",
      "Processed 440 batches -> train_full.dsv\n",
      "Processed 460 batches -> train_full.dsv\n",
      "Processed 480 batches -> train_full.dsv\n",
      "Processed 500 batches -> train_full.dsv\n",
      "Processed 520 batches -> train_full.dsv\n",
      "Processed 540 batches -> train_full.dsv\n",
      "Processed 560 batches -> train_full.dsv\n",
      "Processed 580 batches -> train_full.dsv\n",
      "Processed 600 batches -> train_full.dsv\n",
      "Processed 620 batches -> train_full.dsv\n",
      "Processed 640 batches -> train_full.dsv\n",
      "Processed 660 batches -> train_full.dsv\n",
      "Processed 680 batches -> train_full.dsv\n",
      "Processed 700 batches -> train_full.dsv\n",
      "Processed 720 batches -> train_full.dsv\n",
      "Processed 740 batches -> train_full.dsv\n",
      "Built: /kaggle/working/catboost_cache/train_full.dsv\n"
     ]
    }
   ],
   "source": [
    "build_catboost_dsv(\n",
    "    main_path=train_main_path,\n",
    "    extra_path=train_extra_path,\n",
    "    output_path=TRAIN_DSV,\n",
    "    feature_main_cols=feature_main_cols,\n",
    "    extra_cols_to_add=extra_cols_to_add,\n",
    "    cat_feature_names=cat_feature_names,\n",
    "    key='customer_id',\n",
    "    batch_rows=BATCH_ROWS,\n",
    "    target_path=target_path,\n",
    "    target_cols=target_cols,\n",
    ")\n",
    "\n",
    "print('Built:', TRAIN_DSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984148c9",
   "metadata": {
    "papermill": {
     "duration": 0.003789,
     "end_time": "2026-02-26T17:34:42.049049",
     "exception": false,
     "start_time": "2026-02-26T17:34:42.045260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Inspect training artifacts\n",
    "\n",
    "Print generated training file paths and sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822c3e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:34:42.057657Z",
     "iopub.status.busy": "2026-02-26T17:34:42.057383Z",
     "iopub.status.idle": "2026-02-26T17:34:42.061599Z",
     "shell.execute_reply": "2026-02-26T17:34:42.060851Z"
    },
    "papermill": {
     "duration": 0.010115,
     "end_time": "2026-02-26T17:34:42.062957",
     "exception": false,
     "start_time": "2026-02-26T17:34:42.052842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DSV path: /kaggle/working/catboost_cache/train_full.dsv\n",
      "Train CD path: /kaggle/working/catboost_cache/train.cd\n",
      "Train DSV size (GB): 11.385\n"
     ]
    }
   ],
   "source": [
    "print('Train DSV path:', TRAIN_DSV)\n",
    "print('Train CD path:', TRAIN_CD)\n",
    "print('Train DSV size (GB):', round(TRAIN_DSV.stat().st_size / (1024 ** 3), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257564f",
   "metadata": {
    "papermill": {
     "duration": 0.003631,
     "end_time": "2026-02-26T17:34:42.070345",
     "exception": false,
     "start_time": "2026-02-26T17:34:42.066714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Build test artifacts (streaming)\n",
    "\n",
    "Create the test DSV file with the same feature layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebcfc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:34:42.078761Z",
     "iopub.status.busy": "2026-02-26T17:34:42.078505Z",
     "iopub.status.idle": "2026-02-26T17:44:04.426686Z",
     "shell.execute_reply": "2026-02-26T17:44:04.425813Z"
    },
    "papermill": {
     "duration": 562.354109,
     "end_time": "2026-02-26T17:44:04.428146",
     "exception": false,
     "start_time": "2026-02-26T17:34:42.074037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 batches -> test_full.dsv\n",
      "Processed 40 batches -> test_full.dsv\n",
      "Processed 60 batches -> test_full.dsv\n",
      "Processed 80 batches -> test_full.dsv\n",
      "Processed 100 batches -> test_full.dsv\n",
      "Processed 120 batches -> test_full.dsv\n",
      "Processed 140 batches -> test_full.dsv\n",
      "Processed 160 batches -> test_full.dsv\n",
      "Processed 180 batches -> test_full.dsv\n",
      "Processed 200 batches -> test_full.dsv\n",
      "Processed 220 batches -> test_full.dsv\n",
      "Processed 240 batches -> test_full.dsv\n",
      "Built: /kaggle/working/catboost_cache/test_full.dsv\n"
     ]
    }
   ],
   "source": [
    "build_catboost_dsv(\n",
    "    main_path=test_main_path,\n",
    "    extra_path=test_extra_path,\n",
    "    output_path=TEST_DSV,\n",
    "    feature_main_cols=feature_main_cols,\n",
    "    extra_cols_to_add=extra_cols_to_add,\n",
    "    cat_feature_names=cat_feature_names,\n",
    "    key='customer_id',\n",
    "    batch_rows=BATCH_ROWS,\n",
    ")\n",
    "\n",
    "print('Built:', TEST_DSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc030c97",
   "metadata": {
    "papermill": {
     "duration": 0.004294,
     "end_time": "2026-02-26T17:44:04.437065",
     "exception": false,
     "start_time": "2026-02-26T17:44:04.432771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Inspect test artifacts\n",
    "\n",
    "Print generated test file paths and sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec95d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:44:04.446791Z",
     "iopub.status.busy": "2026-02-26T17:44:04.446249Z",
     "iopub.status.idle": "2026-02-26T17:44:04.450206Z",
     "shell.execute_reply": "2026-02-26T17:44:04.449515Z"
    },
    "papermill": {
     "duration": 0.010446,
     "end_time": "2026-02-26T17:44:04.451640",
     "exception": false,
     "start_time": "2026-02-26T17:44:04.441194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DSV path: /kaggle/working/catboost_cache/test_full.dsv\n",
      "Test CD path: /kaggle/working/catboost_cache/test.cd\n",
      "Test DSV size (GB): 3.776\n"
     ]
    }
   ],
   "source": [
    "print('Test DSV path:', TEST_DSV)\n",
    "print('Test CD path:', TEST_CD)\n",
    "print('Test DSV size (GB):', round(TEST_DSV.stat().st_size / (1024 ** 3), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08c1fe",
   "metadata": {
    "papermill": {
     "duration": 0.004202,
     "end_time": "2026-02-26T17:44:04.460082",
     "exception": false,
     "start_time": "2026-02-26T17:44:04.455880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Train CatBoost model\n",
    "\n",
    "Train the GPU model using file-based pools to avoid RAM spikes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b29451b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:44:04.469477Z",
     "iopub.status.busy": "2026-02-26T17:44:04.469249Z",
     "iopub.status.idle": "2026-02-26T18:14:06.276609Z",
     "shell.execute_reply": "2026-02-26T18:14:06.275888Z"
    },
    "papermill": {
     "duration": 1801.813996,
     "end_time": "2026-02-26T18:14:06.278324",
     "exception": false,
     "start_time": "2026-02-26T17:44:04.464328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4839990\ttotal: 10.8s\tremaining: 3h 18s\n",
      "100:\tlearn: 0.0855802\ttotal: 3m 8s\tremaining: 28m\n",
      "200:\tlearn: 0.0833710\ttotal: 6m 4s\tremaining: 24m 7s\n",
      "300:\tlearn: 0.0820360\ttotal: 8m 57s\tremaining: 20m 47s\n",
      "400:\tlearn: 0.0812207\ttotal: 11m 41s\tremaining: 17m 27s\n",
      "500:\tlearn: 0.0806008\ttotal: 14m 21s\tremaining: 14m 17s\n",
      "600:\tlearn: 0.0801839\ttotal: 16m 53s\tremaining: 11m 12s\n",
      "700:\tlearn: 0.0798082\ttotal: 19m 27s\tremaining: 8m 18s\n",
      "800:\tlearn: 0.0795202\ttotal: 21m 56s\tremaining: 5m 27s\n",
      "900:\tlearn: 0.0792676\ttotal: 24m 23s\tremaining: 2m 40s\n",
      "999:\tlearn: 0.0790025\ttotal: 26m 50s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    data=str(TRAIN_DSV),\n",
    "    column_description=str(TRAIN_CD),\n",
    "    delimiter='\\t',\n",
    "    has_header=False,\n",
    "    thread_count=4,\n",
    ")\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    loss_function='MultiLogloss',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    task_type='GPU',\n",
    "    used_ram_limit='24gb',\n",
    ")\n",
    "\n",
    "model.fit(train_pool)\n",
    "\n",
    "del train_pool\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff535b44",
   "metadata": {
    "papermill": {
     "duration": 0.004681,
     "end_time": "2026-02-26T18:14:06.288102",
     "exception": false,
     "start_time": "2026-02-26T18:14:06.283421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Inference and submission export\n",
    "\n",
    "Generate test predictions and save `submission.parquet` in the required format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af46b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T18:14:06.298959Z",
     "iopub.status.busy": "2026-02-26T18:14:06.298352Z",
     "iopub.status.idle": "2026-02-26T18:14:51.266085Z",
     "shell.execute_reply": "2026-02-26T18:14:51.265207Z"
    },
    "papermill": {
     "duration": 44.974851,
     "end_time": "2026-02-26T18:14:51.267617",
     "exception": false,
     "start_time": "2026-02-26T18:14:06.292766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission.parquet\n"
     ]
    }
   ],
   "source": [
    "test_pool = Pool(\n",
    "    data=str(TEST_DSV),\n",
    "    column_description=str(TEST_CD),\n",
    "    delimiter='\\t',\n",
    "    has_header=False,\n",
    "    thread_count=4,\n",
    ")\n",
    "\n",
    "test_predict = model.predict(test_pool, prediction_type='RawFormulaVal')\n",
    "\n",
    "sample_submit = pd.read_parquet(DATA_DIR / 'sample_submit.parquet')\n",
    "result_df = sample_submit.copy()\n",
    "result_df.iloc[:, 1:] = test_predict\n",
    "result_df['customer_id'] = result_df['customer_id'].astype('int32')\n",
    "result_df.to_parquet('submission.parquet', index=False)\n",
    "\n",
    "print('Saved: submission.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54eeeae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T18:14:51.278642Z",
     "iopub.status.busy": "2026-02-26T18:14:51.278366Z",
     "iopub.status.idle": "2026-02-26T18:14:51.283085Z",
     "shell.execute_reply": "2026-02-26T18:14:51.282411Z"
    },
    "papermill": {
     "duration": 0.011791,
     "end_time": "2026-02-26T18:14:51.284457",
     "exception": false,
     "start_time": "2026-02-26T18:14:51.272666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFinal macro ROC-AUC on public ds = 0,8516956921\\nAt the time of submitting the solution it was 3rd place on leaderboard\\nRun on Kaggle w GPU T4\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Final macro ROC-AUC on public ds = 0,8516956921\n",
    "At the time of submitting the solution it was 3rd place on leaderboard\n",
    "Run on Kaggle w GPU T4\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15786356,
     "datasetId": 9546543,
     "sourceId": 14919853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4149.92529,
   "end_time": "2026-02-26T18:14:53.111152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-26T17:05:43.185862",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
